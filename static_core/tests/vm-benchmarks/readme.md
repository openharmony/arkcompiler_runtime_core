# VM Benchmarks

VMB is a tool for running benchmarks for variety of Virtual Machines,
platforms and languages.

## Prerequisites

Recommended OS to start `vmb` is Linux.
Currently Ubuntu 22.04 is the only fully supported system.
For experimental Windows host support please refer to [this notes](./windows.readme.md).

`python3` (3.7+) with `jinja2` module should be installed.
(`make vmb` will install missed modules).

On Linux host make sure `\time -v uname` works,
and if it doesn't, install `time` package: `sudo apt install time`

Virtual Machine to test should be installed on host and/or device.
See [Notes on setup for platforms](#platforms).

## Quick start

#### Option 1: Using wrapper script

This option is to try VMB without any installation involved.

```shell
# Run all js and ts examples on Node (needs node and tsc):
./run-vmb.sh all -p node_host `pwd`/examples/benchmarks

# Run all only js examples on Node (needs node installed):
./run-vmb.sh all -l js -p node_host `pwd`/examples/benchmarks

# See all options explained:
./run-vmb.sh help

# List all available plugins:
./run-vmb.sh list
```

#### Option2: Build and install vmb module (preferred)

Wrapper script `run-vmb.sh` has all the module functionality,
but it requires absolute paths in cmdline.
Python package, built and installed, provides more flexibility
in dealing with various benchmark sources and repos.
Once installed it exposes single `vmb` command for generating,
running and reporting tests.

```sh
# This command will build and install VMB python module on your machine.
# No root permissions required.
# After that `vmb` cli will appear in your $PATH (if not, please try adding $HOME/.local/bin)

make vmb

# Run all js and ts tests in current dir
vmb all -p node_host

# Run only js tests with tag 'sanity' from 'examples'
vmb all -p node_host -l js -T sanity ./examples
```

#### Usage Example: Compare 2 runs

```shell
export PANDA_BUILD=~/arkcompiler/runtime_core/static_core/build
# Run ets and ts tests on ArkTS in
# 1) interpretation mode
vmb all -p arkts_host --aot-skip-libs \
    --mode=int --report-json=int.json ./examples/benchmarks/
# 2) in JIT mode
vmb all -p arkts_host --aot-skip-libs \
    --mode=jit --report-json=jit.json ./examples/benchmarks/
# Compare results
vmb report --compare int.json jit.json

    Comparison: arkts_host-int vs arkts_host-jit
    ========================================
    Time: 1.99e+05->3.80e+04(better -80.9%); Size: 9.17e+04->9.17e+04(same); RSS: 4.94e+04->5.34e+04(worse +8.1%)
    =============================================================================================================
```

#### Usage example: Re-run failed tests

```shell
# provide option to save lists of failed tests:
vmb all -p node_host --fail-list ./failures.lst <path-to-test-src>
# this will produce two files:
# failures.lst - list file with paths to generate failed tests from
# failures.txt - filter test list to use with --test-list
# Re-run only failed tests:
vmb all -p node_host -fi 1 --test-list=./failures.txt ./failures.lst
```

## Commands:
`vmb` cli supports following commands:
- `gen` - generate benchmark tests from [doclets](#doclet-format)
- `run` - run tests generated by `gen` or [non doclet tests](#non-doclet-tests)
- `all` = `gen` + `run`
- `report` - works with json report (display, compare)
- `list` - show info supported langs and platforms
- `help` - print options for all the commands. (Also `vmb <cmd> --help`)

## Selecting platform
Required `-p` (`--platform`) option to `run` or `all` command should be the name
of plugin from `<vmb-package-root>/plugins/platforms`
or from `<extra-plugins-dir>/platforms` if specified.
F.e. `ark_js_vm_host` - for ArkHz on host machine,
or `arkts_ohos` - for ArkTS on OHOS device

## Selecting language and source files
`gen` command requires `-l` (`--langs`) option.
F.e. `vmb gen -l ets,swift,ts,js ./examples/benchmarks`
will generate benches for all 4 languages in examples.

Then provided to `all` command `--langs` will override langs, supported by platform.
F.e. `vmb all -l js -p v_8_host ./tests` will skip `ts` (typescript)

Source files (doclets) could be overridden
by `--src-langs` option to `all` or `run` command.

Each platform defines languages it can deal with,
and each language defines its source file extension.
Defaults are:

| platform      | langs      | sources                     |
|---------------|------------|-----------------------------|
| `arkts_*`     | `ets`      | `*.ets`                     |
| `ark_js_vm_*` | `ts`       | `*.ts`                      |
| `swift_*`     | `swift`    | `*.swift`                   |
| `v_8_*`       | `ts`, `js` | `*.ts`, `*.js`              |
| `node_*`      | `ts`, `js` | `*.ts`, `*.js`              |
| `interop_s2d` | `ets`      | `*.ets` + `lib*.*`          |
| `interop_d2s` | `ts`, `js` | `*.ts`, `*.js` + `lib*.ets` |
| `interop_d2d` | `ts`, `js` | `*.ts`, `*.js` + `lib*.*`   |
| `hap_s2*`     | `ets`      | `*.ets` + `lib*.*`          |
| `hap_d2*`     | `ts`       | `*.ts` + `lib*.*`           |

## Selecting and filtering tests:
- Any positional argument to `all` or `gen` command would be treated
as path to doclets: `vmb all -p x ./test1/test1.js ./more-tests/ ./even-more`
- Any positional argument to `run` command would be treated
as path to generated tests: `vmb run -p x ./generated`
- Files with `.lst` extension would be treated as lists of paths, relative to `CWD`
- `--tags=sanity,my` (`-T`) will generate and run only tests tagged with `sanity` OR `my`
- `--tests=Test1,'Foo_.*'` (`-t`) will run tests with names matching one of the pattern provided
- `--test-list=/path/to/list.txt` will do the same as `-t`
but reading names and/or name patterns from file (line by line)
- To exclude some tests from run point `--exclude-list` to the file with test names to exclude
- To exclude tests with tags use `-ST` (`--skip-tags`) option. F.e. `--skip-tags=negative,flaky`

Note: `--tests` option expects comma-separated list of exact test names
or regular expression (using python syntax), f.e.:
- `-t foo,bar` will select only `foo` and `bar`
- `-t '[^f]oo',foo` will select both `foo` and `boo` (selection by OR condition)
- `--tests='.*foo(_\d+)?',bar` will select `Test_foo_1 ; foo ; bar` but skip `Test_bar ; bar_1 ; foo_test`

Note: options `--tests` and `--test-list` could be combined (by OR condition) in one command line

## Benchmark's measurement options:
* `-wi` (`--warmup-iters`) controls the number of warmup iterations,
  default is 2.
* `-mi` (`--measure-iters`) controls the number of measurement iterations,
  default is 3.
* `-it` (`--iter-time`) controls the duration of iterations in seconds,
  default is 1.
* `-wt` (`--warmup-time`) controls the duration of warmup iterations in seconds,
  default is 1.
* `-gc` (`--sys-gc-pause`) Non-negative value forces GC (twice)
  and <value> milliseconds sleep before each iteration
  (GC finish can't be guaranteed on all VM's), default is -1 (no forced GC).
* `-fi` (`--fast-iters`) Number of 'fast' iterations
  (no warmup, no tuning cycles).
  Benchmark will run this number of iterations, regardless of time elapsed.

## No run option
`--no-run` (or `-N`) option disables all command's execution.
Instead they will be just printed to console.
This could be used for debug/troubleshooting,
f.e. commands on device could be examined without any device connected
and/or without vm binaries actually installed on host.

## Dry run
`--dry-run` option will cause run to stop after generate and compile steps,
so produced test sources and binaries could be used for debug and analysis.

## Reuse compiled test binaries (experimental)
To re-run tests several times without re-compilation use combination of
`vmb all --skip-cleanup` and `vmb run --skip-compilation`.
First command will go through whole cycle (generation, compilation and run)
and the second one will use compiled binaries.
Note `all` vs `run` and paths used:

```shell
vmb all -p arkts_host -A -v debug --skip-cleanup ./examples/benchmarks/ets

find ./generated/ -name "*.abc"
./generated/benches/bu_ArraySort_baseline/bench_ArraySort_baseline.abc
./generated/benches/bu_ArraySort_sort/bench_ArraySort_sort.abc
./generated/benches/bu_ArraySort_sort_1/bench_ArraySort_sort_1.abc
./generated/benches/bu_ArraySort_baseline_1/bench_ArraySort_baseline_1.abc

vmb run -p arkts_host -A -v debug --skip-cleanup --skip-compilation ./generated/
```


## Custom options
To provide additional options to compiler or virtual machine use
`--<tool>-custom-option`.

F.e. add cpu profiling for `node`:
`vmb all -p node_host --node-custom-option='--cpu-prof' -v debug ./examples/benchmarks/ts/VoidBench.ts`

Note: Option could be provided multiple times in one command for same tool.

F.e. this line `--ark-custom-option='--heap-size-limit=536870912' --ark-custom-option='--print-memory-statistics=true'`
will result in `ark ... --heap-size-limit=536870912 --print-memory-statistics=true ...`

Note: spaces inside option is not allowed. Please use `=` sign.


## Custom paths to executables (experimental)
To override paths to compilers and/or virtual machines
`--<tool>-path` option could be used. F.e. to not use `OHOS_SDK` root:
`--es2abc-path=/path/to/es2abc.exe`.

Or (using aot system binary for device):
 - ArkJsVm: `--ark_aot_compiler-path=/system/bin/ark_aot_compiler`
 - ArkTS: `--paoc-path=/system/bin/ark_aot`

`es2abc`, `es2panda` `ark_js_vm`, `ark`, `ark_aot_compiler`, `paoc` are supported currently.

## Custom script
To run custom shell script after each benchmark test
use `--custom-script` option.
Provided script will run on host or device depending on target platform.
F.e.
```shell
vmb all -p node_host -v debug \
    --custom-script ./examples/scripts/print_health.sh ./examples/benchmarks/ts/
```
These env vars will be set for script:
- `VMB_BU_PATH` - full path to work dir with current test
- `VMB_BU_NAME` - test name as it stored in report

## Reports:
* `--report-json=path.json` to save results in json format,
   this is most detailed report.
  `--report-json-compact` disables prettifying of json.
* `--report-csv=path.csv` to save results in csv format. Only basic info included.

Apart from saved report files text report will always be printed in console.
Also json files could be displayed in separate command:
`vmb report <options> report.json`
For full list of options issue: `vmb report --help`

Note that in saved reports time (or speed) values are _nanoseconds_ (or ns/operation)
and sizes are bytes, while in console output time comes in _seconds_.

Output format could be controlled by `--number-format` option to `run|all|report` command:
- `auto` (default) : `1Âµ 1m 1K ..`
- `nano`: `1n 1000n ..`
- `expo`: `7.00e-02 5.01e+03 ..`

### Results comparison

Reports could compared with `vmb report --compare etalon.json report.json`:

```shell
vmb report --compare \
    [--full] \                  # print all (even same) results to console
    [--json=file.json] \        # save comparison to file
    [--flaky-list=flaky.txt] \  # ignore results for tests from flaky.txt
    [--status-by-compare] \     # set non zero exit code if there were regressions
    [--tags=general,basic] \    # compare only tests tagged general or basic
    [--skip-tags=one,two] \     # skip tests tagged one or two
    report-etalon.json report-to-check.json
```

By default all results are compared with precision (tolerance) of `0.5%`.
This could be tuned by options:

```shell
```shell
vmb report --compare \
    [--tolerance=1.5] \                 # set overall tolerance as 1.5%
    [--tolerance-time=2] \              # set tolerance for time-based metrics as 2%
    [--tolerance-code-size=3] \         # set tolerance for code size-based metrics as 3%
    [--tolerance-rss=4] \               # set tolerance for memory size-based metrics as 4%
    [--tolerance-compile-time=5] \      # set tolerance for compilation time-based metrics as 5%
    [--tolerance-count=6] \             # set tolerance for count-based metrics as 6% (number of passed)
    [--tolerance-list=fine-tuning.csv]  # set tolerances individually for each test
    report-etalon.json report-to-check.json
```
Example of `tolerance-fine-tuning.csv`.
(No tabs allowed in header, only spaces; Comma should follow value immediately)

```csv
name,    time, code_size, rss, compile_time
A_test,  3,    20,        30
B_test,  15,   ,          ,    7.75
```

### Comparison of full test time

Two runs could be compared test by test on total bench time (including compilation, device interactions etc):

```shell
vmb report --compare-meta --tolerance 10 ./{1,2}.json

Full test time comparison (seconds)
===================================
name             | r1 | r2 |
============================
Sample_testSum   |  18|  18|1.77e+01->1.83e+01(same)
VoidBench_test   |   9|  11|9.21e+00->1.13e+01(worse +22.6%)

```

## Log and output:

### Log level
There are several log levels which could be set via `--log-level` option.
- `fatal`: only critical errors will be printed
- `pass`: print single line for each test after it finishes
- `error`
- `warn`
- `info`: default level
- `debug`
- `trace`: most verbose

Each level prints in its own color.
`--no-color` disables color and adds `[%LEVEL]` prefix to each message.

### Print test list
`vmb gen -l ts --show-list ./examples/benchmarks`

### Produce lists for re-run failed tests
`vmb all -p node_host --fail-list ./failures.lst ./examples/benchmarks`

## Extra (custom) plugins:

Providing option `--extra-plugins` you can add any
language, tool, platform or hook.
This parameter should point to the directory containing any combination of
`hooks`, `langs`, `platforms`, `tools` sub-dirs.
As the simplest case you can copy any of the existing plugins
and safely experiment modifying it.
`extra-plugins` has higher 'priority', so any tools, platforms and langs
could be 'overridden' by custom ones.

```sh
# Example:
# 2 demo plugins: Tool=dummy and Platform=dummy_host
mkdir bu_Fake
touch bu_Fake/bench_Fake.txt
vmb run -p dummy_host \
    --extra-plugins=./examples/plugins \
    ./bu_Fake/bench_Fake.txt
```

## Doclet format:
This is a mean for benchmark declaration in source file.
In a single root class it is possible to define any number of benchmarks,
setup method, benchmarks parameters, run options and some additional meta-info.
See examples in `examples/benchamrks` directory.

Using this test format involves separate generation stage (`gen` command).
Resulting benchmark programs will be generated using templates
in `plugins/templates` into `generated` directory
(which could be overridden by `--outdir` option)

Supported doclets are:
* `@State` on a root class which contains benchmarks tests as its methods.
* `@Setup` on an initialization method of a state.
* `@Benchmark` on a method that is measured.
   It should not accept any parameters and
   (preferably) it may return a value which is consumed.
* `@Param  p1 [, p2...]` on a state field.
   Attribute define values to create several benchmarks using same code,
   and all combinations of params.
   Value can be an int, a string or a comma separated list of ints or strings.
* `@Import {x, y...} from ./libX.ext` produces import statement and compile `libX.ext` if needed.
* `@Include ./f.ext` paste contents of `f.ext` into generated source.
  [See Sample.ts](./examples/benchmarks/ts/Sample.ts)
* `@Tags t1 [, t2...]` on a root class or method. List of benchmark tags.
* `@Bugs b1 [, b2...]` on a root class or method. List of associated issues.

JS specific doclets (JSDoc-like) are:
* `@arg {<State name>} <name>` on a benchmark method.
   Links a proper state class to each parameter.
* `@returns {Bool|Char|Int|Float|Obj|Objs}` optionally on a benchmark method.
   Helps to build automatic `Consumer` usage.

## Non-doclet tests

It is possible to write 'free-style' tests which can be ran as is.
VMB will search bench units in directories with `bu_` prefix and files with
`bench_` prefix.
Test result should be reported by program
as `Benchmark result: <TestName> <time in ns>`.

## Platforms

### ArkTS (arkts_*)
On host: env var `PANDA_BUILD` should be set to point to `build` directory
f.e. `~/arkcompiler/runtime_core/static_core/build`

On device: `ark` and `ark_aot` binaries with required libraries should be pushed to device.
Default place is `/data/local/tmp/vmb` and could be configured via `--device-dir`

By default `etsstdlib` will be compiled into native code,
to avoid this use `--aot-skip-libs` (`-A`) option

### Ark JS VM (ark_js_vm_*)
On host: env var `OHOS_SDK` should be set to point to OHOS SDK source directory
inside which `out/*.release` should contain built binaries

On device: `ark_js_vm` and `ark_aot_compiler` should be pushed to device.
Default place is `/data/local/tmp/vmb` and could be configured via `--device-dir`

### V_8
For device platforms: required binaries should be pushed to device.
Default place is `/data/local/tmp/vmb/v_8`.
`/data/local/tmp/vmb` part could be configured via `--device-dir`

### Hap (Ability Package) - Experimental
This platform allows to run `arkts/ets` benchmarks as application ability

##### Prerequisites:
- `PANDA_SDK` env var should point to unpacked Panda OHOS SDK (package dir)
- `OHOS_BASE_SDK_HOME` env var should point to unpacked OHOS SDK
- `hvigorw` should be in PATH or `HVIGORW` env var should point to hvigor script or binary
- `hdc` should be in PATH or `HDC` env var should point to hdc script or binary
- For signing package `HAP_SIGNING_CONFIG` env var should point to json config 
    as in `app/signingConfigs/material` section of `build-profile.json5`

##### Run
```sh
vmb all -p hap -A examples/benchmarks/ets

# --tests-per-batch option could be used to tune amount of benchmarks per one hap package (25 is the default)
```
##### Limitations
- Total test run inside app is limited to 5 sec, so `-wi` is limited to 0..1 and `-mi` to 1..2.
    `-wt` and `it` has no effect for `hap` platform.
- "Macro" benchmarks (i.e. test functions which run longer than 1 sec) won't produce benchmark result.


## Platform Features

| platform       | int-mode | aot-mode | jit-mode | gc-stats | jit-stats | aot-stats | imports |
|----------------|:--------:|:--------:|:--------:|:--------:|:---------:|:---------:|:-------:|
| ark_js_vm_host |    V     |    V     |    V     |   n/a    |    n/a    |    n/a    |    V    |
| ark_js_vm_ohos |    V     |    V     |    V     |   n/a    |    n/a    |    n/a    |    V    |
| arkts_device   |    V     |    V     |    V     |    V     |     V     |     V     |    V    |
| arkts_host     |    V     |    V     |    V     |    V     |     V     |     V     |    V    |
| arkts_ohos     |    V     |    V     |    V     |    V     |     V     |     V     |    V    |
| node_host      |   n/a    |   n/a    |    V     |   n/a    |    n/a    |    n/a    |    V    |
| swift_device   |   n/a    |   n/a    |   n/a    |   n/a    |    n/a    |    n/a    |    X    |
| swift_host     |   n/a    |   n/a    |   n/a    |   n/a    |    n/a    |    n/a    |    X    |
| v_8_device     |    V     |   n/a    |    V     |   n/a    |    n/a    |    n/a    |    V    |
| v_8_host       |    V     |   n/a    |    V     |   n/a    |    n/a    |    n/a    |    V    |
| v_8_ohos       |    V     |   n/a    |    V     |   n/a    |    n/a    |    n/a    |    V    |
| interop_d2s    |    V     |   n/a    |   n/a    |   n/a    |    n/a    |    n/a    |    V    |
| interop_s2d    |    V     |   n/a    |   n/a    |   n/a    |    n/a    |    n/a    |    V    |
| interop_d2d    |    V     |   n/a    |   n/a    |   n/a    |    n/a    |    n/a    |    V    |
| hap_d2s        |    V     |    X     |    X     |   n/a    |    n/a    |    n/a    |    V    |
| hap_s2d        |    V     |    X     |    X     |   n/a    |    n/a    |    n/a    |    V    |
| hap_d2d        |    V     |    X     |    X     |   n/a    |    n/a    |    n/a    |    V    |
| hap_s2s        |    V     |    X     |    X     |   n/a    |    n/a    |    n/a    |    V    |

## Interoperability tests:

Please refer to [this manual](./examples/benchmarks/interop/readme.md) and [examples](./examples/benchmarks/interop).

See also [readme here](../../plugins/ets/tests/benchmarks/interop_js/README.md)

## Self tests and linters

To run all unit tests and linters issue following command:

```sh
make tox
```

See also [development notes](./develop.readme.md)
